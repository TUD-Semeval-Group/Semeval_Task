{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Word2Vec\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the training and validation data\n",
    "train = pd.read_csv('data/public_data/train/track_a/eng.csv')\n",
    "\n",
    "train.head()\n",
    "\n",
    "\n",
    "emotion2label = {'Anger': 0, 'Fear': 1, 'Joy': 2, 'Sadness': 3, 'Surprise': 4}\n",
    "label2emotion = {v: k for k, v in emotion2label.items()}\n",
    "emotions = list(emotion2label.keys())\n",
    "\n",
    "\n",
    "# clean data\n",
    "train_X = train[\"text\"].str.lower()\n",
    "train_X = train_X.str.replace(r'[^\\w\\s]', '') # remove punctuation\n",
    "train_X = train_X.str.replace(r'\\d+', '') # remove numbers\n",
    "train_X = train_X.str.replace(r'\\t', '') # remove tabs\n",
    "train_X = train_X.str.replace(r'\\s+', ' ') # remove extra spaces\n",
    "\n",
    "\n",
    "train_Y = train[emotions].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embedding \n",
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 1024\n",
    "window_size = 200\n",
    "min_word_count = 0\n",
    "workers = 8\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "print([word for sentence in train_X for word in sentence.split(\" \")])\n",
    "tokens = [word_tokenize(word) for sentence in train_X for word in sentence.split(\" \")]\n",
    "\n",
    "\n",
    "# TODO: Train Word2Vec model with 'Word2Vec'\n",
    "word2vec_model = Word2Vec(sentences=[*tokens], vector_size=embedding_dim, window=window_size,\n",
    "                          min_count=min_word_count, workers=workers)\n",
    "print(\"Word2Vec model training complete.\")\n",
    "train_X_emb = [[word2vec_model.wv[word] for word in sentence.split(\" \") if word in word2vec_model.wv] for sentence in train_X]\n",
    "\n",
    "words_to_visualize = [(word) for sentence in train_X[:10] for word in sentence.split(\" \")]\n",
    "print(words_to_visualize)\n",
    "word_vectors = [word2vec_model.wv[word] for word in words_to_visualize]\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(word_vectors)\n",
    "\n",
    "# Plot the word vectors\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, word in enumerate(words_to_visualize):\n",
    "    plt.scatter(reduced_vectors[i][0], reduced_vectors[i][1])\n",
    "    plt.annotate(word, xy=(reduced_vectors[i][0], reduced_vectors[i][1]), fontsize=12)\n",
    "plt.title(\"2D Visualization of Word Embeddings\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "train_X_emb = vectorizer.fit_transform(train_X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_t = torch.Tensor(train_X_emb)\n",
    "train_Y_t = torch.Tensor(train_Y)\n",
    "\n",
    "print(train_X_t.shape, train_Y_t.shape)\n",
    "\n",
    "_ = [print(f' - {e}: {v} ({(100*v/len(train_Y_t)).round()}%)') for e,v in zip(emotions, train_Y_t.sum(axis=0))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
